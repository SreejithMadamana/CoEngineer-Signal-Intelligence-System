using OllamaSharp;
using OllamaSharp.Models;
using System.Text;
using System.Threading.Tasks;
using System.Collections.Generic;

public class LLMMessage
{
    public string Role { get; set; } // "user" or "assistant"
    public string Content { get; set; }
}

public class OllamaService : ILLMService
{
    private readonly OllamaApiClient _client;

    public OllamaService()
    {
        _client = new OllamaApiClient("http://localhost:11434");
    }

    public async Task<string> GetLLMResponseAsync(string prompt)
    {
        var request = new GenerateRequest
        {
            Model = "llama3.2", // or any model you pulled
            Prompt = prompt,
            Options = new RequestOptions
            {
                Temperature = (float?)0.2
            },
            Stream = true
        };

        var sb = new StringBuilder();

        await foreach (var chunk in _client.GenerateAsync(request))
        {
            if (chunk?.Response != null)
            {
                sb.Append(chunk.Response);
            }
        }

        return sb.ToString();
    }

    // NEW: Multi-turn chat support
    public async Task<string> GetLLMChatResponseAsync(List<LLMMessage> messages)
    {
        // Format messages as a single prompt for Ollama
        var promptBuilder = new StringBuilder();
        foreach (var msg in messages)
        {
            promptBuilder.AppendLine($"{msg.Role}: {msg.Content}");
        }
        var prompt = promptBuilder.ToString();

        var request = new GenerateRequest
        {
            Model = "llama3.2",
            Prompt = prompt,
            Options = new RequestOptions
            {
                Temperature = (float?)0.2
            },
            Stream = true
        };

        var sb = new StringBuilder();
        await foreach (var chunk in _client.GenerateAsync(request))
        {
            if (chunk?.Response != null)
            {
                sb.Append(chunk.Response);
            }
        }
        return sb.ToString();
    }
}